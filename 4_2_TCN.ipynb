{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import neurokit2 as nk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tcn import TCN\n",
    "\n",
    "from Modules.Feature_Extraction_temporal_domain import *\n",
    "from Modules.tcnae import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_csv(filepath):\n",
    "    f = open(filepath)\n",
    "    data = json.load(f)\n",
    "    return data\n",
    "\n",
    "#ecg = read_csv('Preprocessed_data/ecg/ecg_unfiltered.txt')\n",
    "ecg_filt = read_csv('Preprocessed_data/ecg/ecg_filtered.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#remove first and last few minutes\n",
    "for patient in ecg_filt:\n",
    "    ecg_filt[patient] = ecg_filt[patient][5000:700000]\n",
    "    #ecg[patient] = ecg[patient][5000:763000]\n",
    "\n",
    "    ecg_filt[patient] = nk.ecg_clean(ecg_filt[patient], sampling_rate=62.475)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "duration = int(10*62.4725)\n",
    "segments_ecg = []\n",
    "\n",
    "for patient in ecg_filt:\n",
    "    values = ecg_filt[patient]\n",
    "    for i in range(0,len(values) - duration + 1, duration):\n",
    "        segments_ecg.append(values[i : (i + duration)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(segments_ecg,test_size=0.3,shuffle = True)\n",
    "test_df, val_df = train_test_split(val_df,test_size=0.5,shuffle = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature extraction from the temporal domain\n",
    "Morphological/time domain features are commonly used to train the machine learning modelsWe will extract some of the common ECG features in the literature. Some features can be calculated using only R-peak locations while others requires information on the other fiducial points (P, Q, S and T waves).\n",
    "\n",
    "#### Features from R peaks\n",
    "\n",
    "The ___from_Rpeaks___ function calculates morphological features using R-peak locations. These features are:\n",
    "\n",
    "'RR1': Current RR interval\n",
    "'RRm': Mean of RR0, RR1 and RR2\n",
    "\n",
    "#### Features from P, Q, R, S, T waves\n",
    "\n",
    "The ___from_waves___ function calculates morphological features using locations of P, Q, R, S, T waves. These features are:\n",
    "\n",
    "'t_PR': Time between P and R peak locations\n",
    "'t_QS': Time between Q and S peak locations\n",
    "'t_QT':Time between Q and T peak locations\n",
    "'t_PT_QS': Ratio of t_PT to t_QS\n",
    "'t_QT_QS': Ratio of t_QT to t_QS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# def get_features(segments_ecg, sampling_rate, num_samples):\n",
    "#\n",
    "#     all_filtered_signals = []\n",
    "#     all_locs_peaks = []\n",
    "#     all_p_peaks_locs = []\n",
    "#     all_q_peaks_locs = []\n",
    "#     all_features_rpeaks = []\n",
    "#     all_features_waves = []\n",
    "#\n",
    "#     for i in range(num_samples):\n",
    "#             try:\n",
    "#                 filtered_ecg, locs_peaks, p_peaks_locs, q_peaks_locs, features_rpeaks, features_waves = find_features(segments_ecg[i], sampling_rate)\n",
    "#                 all_filtered_signals.append(filtered_ecg)\n",
    "#                 all_locs_peaks.append(locs_peaks)\n",
    "#                 all_p_peaks_locs.append(p_peaks_locs)\n",
    "#                 all_q_peaks_locs.append(q_peaks_locs)\n",
    "#                 all_features_rpeaks.append(features_rpeaks)\n",
    "#                 all_features_waves.append(features_waves)\n",
    "#             except:\n",
    "#                 continue\n",
    "#\n",
    "#     features_waves_df = pd.DataFrame(all_features_waves)\n",
    "#     features_waves_df  = features_waves_df.fillna(0)\n",
    "#     features_waves_reduced = features_waves_df[[\"ecg_t_PR\", \"ecg_t_QS\", \"ecg_t_QT\", \"ecg_t_PT_QS\", \"ecg_t_QT_QS\"]]\n",
    "#\n",
    "#     features_rpeaks_df = pd.DataFrame(all_features_rpeaks)\n",
    "#     features_rpeaks_df  = features_rpeaks_df.fillna(0)\n",
    "#\n",
    "#     features_waves_reduced[\"ecg_RR1\"] = features_rpeaks_df[[\"ecg_RR1\"]]\n",
    "#     features_waves_reduced[\"ecg_RRm\"] = features_rpeaks_df[[\"ecg_RRm\"]]\n",
    "#\n",
    "#     features_waves_reduced = features_waves_reduced.to_numpy()\n",
    "#\n",
    "#     return features_waves_reduced, all_filtered_signals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# sampling_rate = 62.475\n",
    "#\n",
    "# features_x_train, train_df = get_features(train_df, sampling_rate, 1000)\n",
    "# features_x_val, val_df = get_features(val_df, sampling_rate, 250)\n",
    "# features_x_test, test_df = get_features(test_df, sampling_rate, 250)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape:  (18124, 624, 1)\n",
      "Val input shape:  (3884, 624, 1)\n",
      "Test input shape:  (3884, 624, 1)\n"
     ]
    }
   ],
   "source": [
    "train_df = np.stack(train_df)\n",
    "val_df = np.stack(val_df)\n",
    "test_df = np.stack(test_df)\n",
    "\n",
    "x_train = train_df.reshape(train_df.shape[0],train_df.shape[1], 1)\n",
    "x_val = val_df.reshape(val_df.shape[0],val_df.shape[1], 1)\n",
    "x_test = test_df.reshape(test_df.shape[0],test_df.shape[1], 1)\n",
    "\n",
    "print(\"Training input shape: \", x_train.shape)\n",
    "print(\"Val input shape: \", x_val.shape)\n",
    "print(\"Test input shape: \", x_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from Modules.utilities import *\n",
    "import numpy\n",
    "from tcn import TCN\n",
    "import time\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import UpSampling1D\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas\n",
    "\n",
    "class TCNAE:\n",
    "    \"\"\"\n",
    "    A class used to represent the Temporal Convolutional Autoencoder (TCN-AE).\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model : xxtypexx\n",
    "        The TCN-AE model.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    build_model(verbose = 1)\n",
    "        Builds the model\n",
    "    \"\"\"\n",
    "\n",
    "    model = None\n",
    "\n",
    "    def __init__(self,\n",
    "                 ts_dimension = 1,\n",
    "                 dilations = (1, 2, 4, 8, 16),\n",
    "                 nb_filters = 20,\n",
    "                 kernel_size = 20,\n",
    "                 nb_stacks = 1,\n",
    "                 padding = 'same',\n",
    "                 dropout_rate = 0.00,\n",
    "                 filters_conv1d = 8,\n",
    "                 activation_conv1d = 'linear',\n",
    "                 latent_sample_rate = 42,\n",
    "                 pooler = AveragePooling1D,\n",
    "                 lr = 0.001,\n",
    "                 conv_kernel_init = 'glorot_normal',\n",
    "                 loss = 'logcosh',\n",
    "                 use_early_stopping = False,\n",
    "                 error_window_length = 128,\n",
    "                 verbose = 1\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts_dimension : int\n",
    "            The dimension of the time series (default is 1)\n",
    "        dilations : tuple\n",
    "            The dilation rates used in the TCN-AE model (default is (1, 2, 4, 8, 16))\n",
    "        nb_filters : int\n",
    "            The number of filters used in the dilated convolutional layers. All dilated conv. layers use the same number of filters (default is 20)\n",
    "        \"\"\"\n",
    "\n",
    "        self.ts_dimension = ts_dimension\n",
    "        self.dilations = dilations\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.padding = padding\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.filters_conv1d = filters_conv1d\n",
    "        self.activation_conv1d = activation_conv1d\n",
    "        self.latent_sample_rate = latent_sample_rate\n",
    "        self.pooler = pooler\n",
    "        self.lr = lr\n",
    "        self.conv_kernel_init = conv_kernel_init\n",
    "        self.loss = loss\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "        self.error_window_length = error_window_length\n",
    "\n",
    "        # build the model\n",
    "        self.build_model(verbose = verbose)\n",
    "\n",
    "\n",
    "    def build_model(self, verbose = 1):\n",
    "        \"\"\"Builds the TCN-AE model.\n",
    "\n",
    "        If the argument `verbose` isn't passed in, the default verbosity level is used.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : str, optional\n",
    "            The verbosity level (default is 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        KerasXYZType\n",
    "        Todo\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            If ...\n",
    "        \"\"\"\n",
    "\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        sampling_factor = self.latent_sample_rate\n",
    "        i = Input(batch_shape=(None, None, self.ts_dimension))\n",
    "\n",
    "        # Put signal through TCN. Output-shape: (batch,sequence length, nb_filters)\n",
    "        tcn_enc = TCN(nb_filters=self.nb_filters, kernel_size=self.kernel_size, nb_stacks=self.nb_stacks, dilations=self.dilations,\n",
    "                      padding=self.padding, use_skip_connections=True, dropout_rate=self.dropout_rate, return_sequences=True,\n",
    "                      kernel_initializer=self.conv_kernel_init, name='tcn-enc')(i)\n",
    "\n",
    "        # Now, adjust the number of channels...\n",
    "        enc_flat = Conv1D(filters=self.filters_conv1d, kernel_size=1, activation=self.activation_conv1d, padding=self.padding)(tcn_enc)\n",
    "\n",
    "        ## Do some average (max) pooling to get a compressed representation of the time series (e.g. a sequence of length 8)\n",
    "        enc_pooled = self.pooler(pool_size=sampling_factor, strides=None, padding='valid', data_format='channels_last')(enc_flat)\n",
    "\n",
    "        # If you want, maybe put the pooled values through a non-linear Activation\n",
    "        enc_out = Activation(\"linear\")(enc_pooled)\n",
    "\n",
    "        # Now we should have a short sequence, which we will upsample again and then try to reconstruct the original series\n",
    "        dec_upsample = UpSampling1D(size=sampling_factor)(enc_out)\n",
    "\n",
    "        dec_reconstructed = TCN(nb_filters=self.nb_filters, kernel_size=self.kernel_size, nb_stacks=self.nb_stacks, dilations=self.dilations,\n",
    "                                padding=self.padding, use_skip_connections=True, dropout_rate=self.dropout_rate, return_sequences=True,\n",
    "                                kernel_initializer=self.conv_kernel_init, name='tcn-dec')(dec_upsample)\n",
    "\n",
    "        # Put the filter-outputs through a dense layer finally, to get the reconstructed signal\n",
    "        o = Dense(self.ts_dimension, activation='linear')(dec_reconstructed)\n",
    "\n",
    "        model = Model(inputs=[i], outputs=[o])\n",
    "\n",
    "        adam = optimizers.Adam(lr=self.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, amsgrad=True)\n",
    "        model.compile(loss=self.loss, optimizer=adam, metrics=[self.loss])\n",
    "        if verbose > 1:\n",
    "            model.summary()\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, train_X, train_Y, batch_size=32, epochs=40, verbose = 1):\n",
    "        my_callbacks = None\n",
    "        if self.use_early_stopping:\n",
    "            my_callbacks = [EarlyStopping(monitor='val_loss', patience=2, min_delta=1e-4, restore_best_weights=True)]\n",
    "\n",
    "        keras_verbose = 0\n",
    "        if verbose > 0:\n",
    "            print(\"> Starting the Training...\")\n",
    "            keras_verbose = 2\n",
    "        start = time.time()\n",
    "        history = self.model.fit(train_X, train_Y,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_split=0.001,\n",
    "                                 shuffle=True,\n",
    "                                 callbacks=my_callbacks,\n",
    "                                 verbose=keras_verbose)\n",
    "        if verbose > 0:\n",
    "            print(\"> Training Time :\", round(time.time() - start), \"seconds.\")\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        X_rec =  self.model.predict(test_X)\n",
    "\n",
    "        # do some padding in the end, since not necessarily the whole time series is reconstructed\n",
    "        X_rec = numpy.pad(X_rec, ((0,0),(0, test_X.shape[1] - X_rec.shape[1] ), (0,0)), 'constant')\n",
    "        E_rec = (X_rec - test_X).squeeze()\n",
    "        Err = slide_window(pandas.DataFrame(E_rec), self.error_window_length, verbose = 0)\n",
    "        Err = Err.reshape(-1, Err.shape[-1]*Err.shape[-2])\n",
    "        sel = numpy.random.choice(range(Err.shape[0]),int(Err.shape[0]*0.98))\n",
    "        mu = numpy.mean(Err[sel], axis=0)\n",
    "        cov = numpy.cov(Err[sel], rowvar = False)\n",
    "        sq_mahalanobis = mahalanobis_distance(X=Err[:], cov=cov, mu=mu)\n",
    "        # moving average over mahalanobis distance. Only slightly smooths the signal\n",
    "        anomaly_score = numpy.convolve(sq_mahalanobis, numpy.ones((50,))/50, mode='same')\n",
    "        anomaly_score = numpy.sqrt(anomaly_score)\n",
    "        return anomaly_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 1)]         0         \n",
      "                                                                 \n",
      " tcn-enc (TCN)               (None, None, 20)          72640     \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 8)           168       \n",
      "                                                                 \n",
      " average_pooling1d (Average  (None, None, 8)           0         \n",
      " Pooling1D)                                                      \n",
      "                                                                 \n",
      " activation (Activation)     (None, None, 8)           0         \n",
      "                                                                 \n",
      " up_sampling1d (UpSampling1  (None, None, 8)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " tcn-dec (TCN)               (None, None, 20)          75580     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 1)           21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148409 (579.72 KB)\n",
      "Trainable params: 148409 (579.72 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Build and compile the model\n",
    "#\n",
    "tcn_ae = TCNAE() # Use the parameters specified in the paper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting the Training...\n",
      "(18124, 624, 1)\n",
      "(18124, 624, 1)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py\", line 2057, in log_cosh\n        return backend.mean(_logcosh(y_pred - y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 588 and 624 for '{{node log_cosh/sub}} = Sub[T=DT_FLOAT](model/dense/BiasAdd, IteratorGetNext:1)' with input shapes: [?,588,1], [?,624,1].\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtcn_ae\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 157\u001B[0m, in \u001B[0;36mTCNAE.fit\u001B[1;34m(self, train_X, train_Y, batch_size, epochs, verbose)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_X\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_Y\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m--> 157\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_Y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmy_callbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeras_verbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m> Training Time :\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mround\u001B[39m(time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseconds.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileodb7_z4r.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py\", line 2057, in log_cosh\n        return backend.mean(_logcosh(y_pred - y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 588 and 624 for '{{node log_cosh/sub}} = Sub[T=DT_FLOAT](model/dense/BiasAdd, IteratorGetNext:1)' with input shapes: [?,588,1], [?,624,1].\n"
     ]
    }
   ],
   "source": [
    "tcn_ae.fit(x_train, x_train, batch_size=32, epochs=10, verbose=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
